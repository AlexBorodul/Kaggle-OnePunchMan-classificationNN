{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1d21f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, torch, cv2, random, albumentations as A, nibabel as nib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt; from glob import glob;\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from albumentations.pytorch import ToTensorV2; from PIL import Image\n",
    "from torchvision import transforms as tfs\n",
    "from torchvision import transforms as tfs\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9853c435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9090f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSegmentationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root, transformations = None):\n",
    "\n",
    "        im_nii_paths = sorted(glob(f\"{root}/Task03_Liver_rs/imagesTr/*.nii\"))\n",
    "        gt_nii_paths = sorted(glob(f\"{root}/Task03_Liver_rs/labelsTr/*.nii\"))\n",
    "        \n",
    "        self.ims, self.gts = self.get_slices(im_nii_paths, gt_nii_paths)\n",
    "        self.transformations = transformations\n",
    "        self.n_cls = 2\n",
    "        \n",
    "        assert len(self.ims) == len(self.gts)\n",
    "        \n",
    "    def __len__(self): return len(self.ims)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        im, gt = self.ims[idx], self.gts[idx]\n",
    "        if self.transformations: im, gt = self.apply_transformations(im, gt)\n",
    "        \n",
    "        # For visualization purposes\n",
    "        im = self.preprocess_im(im)\n",
    "        # For the cases when label equals to 2; to avoid CE Loss error\n",
    "        gt[gt > 1] = 1 \n",
    "        \n",
    "        return im.float(), gt.unsqueeze(0).long()\n",
    "        \n",
    "    def preprocess_im(self, im): \n",
    "        \n",
    "        max_val = torch.max(im)\n",
    "        im[im < 0] = 0\n",
    "        \n",
    "        return im / max_val\n",
    "    \n",
    "    def get_slices(self, im_nii_paths, gt_nii_paths): \n",
    "        \n",
    "        ims, gts = [], []\n",
    "        \n",
    "        for index, (im_nii, gt_nii) in enumerate(zip(im_nii_paths, gt_nii_paths)):\n",
    "            if index == 50: break\n",
    "            print(f\"nifti file number {index + 1} is being converted...\")\n",
    "            nii_im_data, nii_gt_data  = self.read_nii(im_nii, gt_nii)\n",
    "            \n",
    "            for idx, (im, gt) in enumerate(zip(nii_im_data, nii_gt_data)):\n",
    "                if len(np.unique(gt)) == 2: ims.append(im); gts.append(gt)\n",
    "        \n",
    "        return ims, gts\n",
    "\n",
    "    def read_nii(self, im, gt): return nib.load(im).get_fdata().transpose(2, 1, 0), nib.load(gt).get_fdata().transpose(2, 1, 0)\n",
    "    \n",
    "    def apply_transformations(self, im, gt): transformed = self.transformations(image = im, mask = gt); return transformed[\"image\"], transformed[\"mask\"]\n",
    "\n",
    "def get_dls(root, transformations, bs, split = [0.9, 0.05, 0.05], ns = 4):\n",
    "        \n",
    "    assert sum(split) == 1., \"Sum of the split must be exactly 1\"\n",
    "    \n",
    "    ds = CustomSegmentationDataset(root = root, transformations = transformations)\n",
    "    n_cls = ds.n_cls\n",
    "    \n",
    "    tr_len = int(len(ds) * split[0])\n",
    "    val_len = int(len(ds) * split[1])\n",
    "    test_len = len(ds) - (tr_len + val_len)\n",
    "    \n",
    "    # Data split\n",
    "    tr_ds, val_ds, test_ds = torch.utils.data.random_split(ds, [tr_len, val_len, test_len])\n",
    "        \n",
    "    print(f\"\\nThere are {len(tr_ds)} number of images in the train set\")\n",
    "    print(f\"There are {len(val_ds)} number of images in the validation set\")\n",
    "    print(f\"There are {len(test_ds)} number of images in the test set\\n\")\n",
    "    \n",
    "    # Get dataloaders\n",
    "    tr_dl  = DataLoader(dataset = tr_ds, batch_size = bs, shuffle = True, num_workers = ns)\n",
    "    val_dl = DataLoader(dataset = val_ds, batch_size = bs, shuffle = False, num_workers = ns)\n",
    "    test_dl = DataLoader(dataset = test_ds, batch_size = bs, shuffle = False, num_workers = ns)\n",
    "    \n",
    "    return tr_dl, val_dl, test_dl, n_cls, ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ddb0d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 0 number of images in the train set\n",
      "There are 0 number of images in the validation set\n",
      "There are 0 number of images in the test set\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m mean, std, im_h, im_w \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], [\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m], \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m      3\u001b[0m trans \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mCompose( [A\u001b[38;5;241m.\u001b[39mResize(im_h, im_w), ToTensorV2(transpose_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) ])\n\u001b[1;32m----> 4\u001b[0m tr_dl, val_dl, test_dl, n_cls, ds \u001b[38;5;241m=\u001b[39m \u001b[43mget_dls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 72\u001b[0m, in \u001b[0;36mget_dls\u001b[1;34m(root, transformations, bs, split, ns)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_ds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m number of images in the test set\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Get dataloaders\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m tr_dl  \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtr_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m val_dl \u001b[38;5;241m=\u001b[39m DataLoader(dataset \u001b[38;5;241m=\u001b[39m val_ds, batch_size \u001b[38;5;241m=\u001b[39m bs, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, num_workers \u001b[38;5;241m=\u001b[39m ns)\n\u001b[0;32m     74\u001b[0m test_dl \u001b[38;5;241m=\u001b[39m DataLoader(dataset \u001b[38;5;241m=\u001b[39m test_ds, batch_size \u001b[38;5;241m=\u001b[39m bs, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, num_workers \u001b[38;5;241m=\u001b[39m ns)\n",
      "File \u001b[1;32mc:\\Users\\Алексей\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:350\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 350\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Алексей\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:143\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "root = \"/kaggle/input/3d-liver-segmentation\"\n",
    "mean, std, im_h, im_w = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225], 128, 128\n",
    "trans = A.Compose( [A.Resize(im_h, im_w), ToTensorV2(transpose_mask = True) ])\n",
    "tr_dl, val_dl, test_dl, n_cls, ds = get_dls(root = root, transformations = trans, bs = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf5a56ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mds\u001b[49m[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(),zorder\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(ds[i][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(),zorder\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m      5\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(1,100):\n",
    "    \n",
    "    plt.imshow(ds[i][0].squeeze(),zorder=0)\n",
    "    plt.imshow(ds[i][1].squeeze(),zorder=1, alpha=0.5)\n",
    "    plt.show()\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5af1d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def ans_show(model, batch, num=3):\n",
    "    model.eval()\n",
    "\n",
    "    as_img = T.ToPILImage()\n",
    "    \n",
    "    i = 0\n",
    "    for x, y in zip(batch[0], batch[1]):\n",
    "        if i >= num:\n",
    "            break\n",
    "            \n",
    "        img_x = x.unsqueeze(0)\n",
    "        img_y = y\n",
    "        \n",
    "        pred = model(img_x.cuda())\n",
    "        pred = F.sigmoid(pred.detach()).cpu().numpy()[0].transpose(1,2,0)\n",
    "        \n",
    "        img_np = img.detach().cpu().numpy()[0].transpose(1,2,0)\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 3, figsize=(15, 8))\n",
    "        ax[0].imshow(img_np)\n",
    "        ax[1].imshow(as_img(y.to(torch.uint8)))\n",
    "        ax[2].imshow(pred)\n",
    "\n",
    "        plt.show()\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ea80f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNA(nn.Module):\n",
    "    def __init__(self, in_nc, out_nc, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_nc, out_nc, 3, stride=stride, padding=1, bias=False)\n",
    "        self.norm = nn.BatchNorm2d(out_nc)\n",
    "        self.act = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.norm(out)\n",
    "        out = self.act(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb7580e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, in_nc, inner_nc, out_nc, inner_block=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = CNA(in_nc, inner_nc, stride=2)\n",
    "        self.conv2 = CNA(inner_nc, inner_nc)\n",
    "        self.inner_block = inner_block\n",
    "        self.conv3 = CNA(inner_nc, inner_nc)\n",
    "        self.conv_cat = nn.Conv2d(inner_nc+in_nc, out_nc, 3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _,_,h,w = x.shape\n",
    "        \n",
    "        inner = self.conv1(x)\n",
    "        inner = self.conv2(inner)\n",
    "        #print(inner.shape)\n",
    "        if self.inner_block is not None:\n",
    "            inner = self.inner_block(inner)\n",
    "        inner = self.conv3(inner)\n",
    "        \n",
    "        inner = F.interpolate(inner, size=(h,w), mode='bilinear')\n",
    "        inner = torch.cat((x, inner), axis=1)\n",
    "        out = self.conv_cat(inner)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12688fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_nc=1, nc=32, out_nc=1, num_downs=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cna1 = CNA(in_nc, nc)\n",
    "        self.cna2 = CNA(nc, nc)\n",
    "        \n",
    "        unet_block = None\n",
    "        for i in range(num_downs-3):\n",
    "            unet_block = UnetBlock(8*nc, 8*nc, 8*nc, unet_block)\n",
    "        unet_block = UnetBlock(4*nc, 8*nc, 4*nc, unet_block)\n",
    "        unet_block = UnetBlock(2*nc, 4*nc, 2*nc, unet_block)\n",
    "        self.unet_block = UnetBlock(nc, 2*nc, nc, unet_block)\n",
    "        \n",
    "        self.cna3 = CNA(nc, nc)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(nc, out_nc, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cna1(x)\n",
    "        out = self.cna2(out)\n",
    "        out = self.unet_block(out)\n",
    "        out = self.cna3(out)\n",
    "        out = self.conv_last(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20f0117c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Алексей\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:414\u001b[0m, in \u001b[0;36mget_device_name\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    403\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the name of a device.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32mc:\\Users\\Алексей\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:444\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \n\u001b[0;32m    437\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 444\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    445\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[1;32mc:\\Users\\Алексей\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31074725",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = UNET(in_nc=1, nc=32, out_nc=1, num_downs=5)\n",
    "unet_model = unet_model.to(device)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(unet_model.parameters(), lr=1e-3, betas=(0.9, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b4bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    loss_val = 0\n",
    "    acc_val = 0\n",
    "    for sample in (pbar := tqdm(tr_dl)):\n",
    "        img, mask = sample\n",
    "        img = img.to(device)\n",
    "        mask = mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = unet_model(img)\n",
    "        loss = loss_fn(pred, mask.float())\n",
    "\n",
    "        loss.backward()\n",
    "        loss_item = loss.item()\n",
    "        loss_val += loss_item\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "    #pbar.set_description(f'loss: {loss_item:.5f}\\tlr: {scheduler.get_last_lr}')\n",
    "    print(f'{loss_val/len(tr_dl)}\\t lr: {scheduler.get_last_lr()}')\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eae040",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_show(unet_model, next(iter(tr_dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02a36bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "090f2fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff2a0f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры\n",
    "IMG_SIZE = 224  # Размер изображений\n",
    "BATCH_SIZE = 8  # Размер батча\n",
    "NUM_EPOCHS = 10  # Количество эпох обучения\n",
    "LEARNING_RATE = 0.001  # Скорость обучения\n",
    "\n",
    "# Определение путей к данным\n",
    "train_dir = 'train'\n",
    "test_dir = 'submission'\n",
    "train_csv_path = 'train.csv'\n",
    "test_csv_path = 'submission.csv'\n",
    "submission_csv_path = 'submission.csv'\n",
    "\n",
    "# Определение классов\n",
    "classes = ['Гароу', 'Генос', 'Сайтама', 'Соник', 'Татсумаки', 'Фубуки']\n",
    "mean, std, im_h, im_w = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225], 128, 128\n",
    "\"\"\"transform = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])# Преобразования для изображений\"\"\"\n",
    "\"\"\"transform_train = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    #transforms.Lambda(lambda x: x.repeat(3, 1, 1))\n",
    "])\"\"\"\n",
    "\n",
    "\"\"\"transform_test = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Lambda(lambda x: x.repeat(3, 1, 1))\n",
    "])\"\"\"\n",
    "\n",
    "class AnimeDataset(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.df = pd.read_csv(csv_path, encoding='utf-8')  # Изменить кодировку на utf-8\n",
    "        self.transform = transform\n",
    "        self.classes = ['Гароу', 'Генос', 'Сайтама', 'Соник', 'Татсумаки', 'Фубуки']  # Добавлен Генос \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.iloc[idx]['path']\n",
    "        print(f\"Path: {path}\")\n",
    "        image = torchvision.io.read_image(path)\n",
    "        print(f\"Image type before: {type(image)}\")\n",
    "        # Преобразовать тензор PyTorch в PIL.Image\n",
    "        image = transforms.ToPILImage()(image) \n",
    "        print(f\"Image type after: {type(image)}\")\n",
    "        # Преобразовать PIL.Image в numpy.ndarray\n",
    "        image = np.array(image)\n",
    "        print(f\"Image type after np.array: {type(image)}\")\n",
    "        # Переставить размерность (H, W, C) в (C, H, W)\n",
    "        image = np.transpose(image, (2, 0, 1))  # Исправленная строка\n",
    "        print(f\"Image shape after transpose: {image.shape}\")\n",
    "\n",
    "        if self.transform:\n",
    "            print(f\"Transforming...\")\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "            print(f\"Image type after transform: {type(image)}\")\n",
    "        label = self.classes.index(self.df.iloc[idx]['class'])\n",
    "        print(f\"Label: {label}\")\n",
    "        return image, label\n",
    "    \n",
    "    def collate_fn(data):\n",
    "        img, bbox = data\n",
    "        zipped = zip(img, bbox)\n",
    "        return list(zipped)\n",
    "\n",
    "mean = [0.485, 0.456, 0.406, 0.5] * 141  # 4 значения, умноженные на 141 = 564 \n",
    "std = [0.229, 0.224, 0.225, 0.5] * 141  # 4 значения, умноженные на 141 = 564 \n",
    "transform = A.Compose([\n",
    "    A.ToFloat(),  # Преобразовать тензор в тип float32\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(mean=mean, std=std),  # Используйте правильные значения mean и std\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad8a7068",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = AnimeDataset(train_csv_path, transform=transform)\n",
    "test_data = AnimeDataset(test_csv_path, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f70b1905",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, classes): \n",
    "        super(Net, self).__init__()\n",
    "        self.resnet = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT) \n",
    "        self.resnet.fc = nn.Identity() \n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = True \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(self.resnet.layer4[2].conv3.out_channels, len(classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "train_classes = train_data.classes\n",
    "model = Net(classes=train_classes)\n",
    "model = model.float().to(device, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90e76f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: train/Гароу/3e7270b8e405ce37791f880814b751a2.jpg\n",
      "Image type before: <class 'torch.Tensor'>\n",
      "Image type after: <class 'PIL.Image.Image'>\n",
      "Image type after np.array: <class 'numpy.ndarray'>\n",
      "Image shape after transpose: (3, 564, 564)\n",
      "Transforming...\n",
      "Image type after transform: <class 'numpy.ndarray'>\n",
      "Label: 0\n",
      "Path: train/Фубуки/3dcef83df0abfb4c01993169e61d687e.jpg\n",
      "Image type before: <class 'torch.Tensor'>\n",
      "Image type after: <class 'PIL.Image.Image'>\n",
      "Image type after np.array: <class 'numpy.ndarray'>\n",
      "Image shape after transpose: (3, 705, 564)\n",
      "Transforming...\n",
      "Image type after transform: <class 'numpy.ndarray'>\n",
      "Label: 5\n",
      "Path: train/Соник/9e4dd99d85dfdd16c543ae3158ab97eb.jpg\n",
      "Image type before: <class 'torch.Tensor'>\n",
      "Image type after: <class 'PIL.Image.Image'>\n",
      "Image type after np.array: <class 'numpy.ndarray'>\n",
      "Image shape after transpose: (3, 564, 564)\n",
      "Transforming...\n",
      "Image type after transform: <class 'numpy.ndarray'>\n",
      "Label: 3\n",
      "Path: train/Татсумаки/0ec0a9f4b427373a1776f3dbd6753c3e.jpg\n",
      "Image type before: <class 'torch.Tensor'>\n",
      "Image type after: <class 'PIL.Image.Image'>\n",
      "Image type after np.array: <class 'numpy.ndarray'>\n",
      "Image shape after transpose: (3, 624, 563)\n",
      "Transforming...\n",
      "Image type after transform: <class 'numpy.ndarray'>\n",
      "Label: 4\n",
      "Path: train/Соник/1d795e2b6c29bb329b4049c82fa6410e.jpg\n",
      "Image type before: <class 'torch.Tensor'>\n",
      "Image type after: <class 'PIL.Image.Image'>\n",
      "Image type after np.array: <class 'numpy.ndarray'>\n",
      "Image shape after transpose: (3, 435, 377)\n",
      "Transforming...\n",
      "Image type after transform: <class 'numpy.ndarray'>\n",
      "Label: 3\n",
      "Path: train/Соник/0e7a898513d1b15ff3d301700ebd53b9.jpg\n",
      "Image type before: <class 'torch.Tensor'>\n",
      "Image type after: <class 'PIL.Image.Image'>\n",
      "Image type after np.array: <class 'numpy.ndarray'>\n",
      "Image shape after transpose: (3, 386, 386)\n",
      "Transforming...\n",
      "Image type after transform: <class 'numpy.ndarray'>\n",
      "Label: 3\n",
      "Path: train/Гароу/2f9f3b7cd7a52c91f36fb6b46f01081f.jpg\n",
      "Image type before: <class 'torch.Tensor'>\n",
      "Image type after: <class 'PIL.Image.Image'>\n",
      "Image type after np.array: <class 'numpy.ndarray'>\n",
      "Image shape after transpose: (3, 830, 540)\n",
      "Transforming...\n",
      "Image type after transform: <class 'numpy.ndarray'>\n",
      "Label: 0\n",
      "Path: train/Татсумаки/1dc60dfd9f129cacce787376a77e842a.jpg\n",
      "Image type before: <class 'torch.Tensor'>\n",
      "Image type after: <class 'PIL.Image.Image'>\n",
      "Image type after np.array: <class 'numpy.ndarray'>\n",
      "Image shape after transpose: (3, 569, 400)\n",
      "Transforming...\n",
      "Image type after transform: <class 'numpy.ndarray'>\n",
      "Label: 4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [224, 224, 564] at entry 0 and [224, 224, 563] at entry 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mddddd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Алексей\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Алексей\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Алексей\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Алексей\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Алексей\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Алексей\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Алексей\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\Алексей\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:222\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[1;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Алексей\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\Алексей\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [224, 224, 564] at entry 0 and [224, 224, 563] at entry 3"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999))\n",
    "torch.cuda.empty_cache()\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        print('ddddd', images.type())\n",
    "        optimizer.zero_grad()\n",
    "        # размерность ожно пошалить\n",
    "        images = images.permute(0, 3, 1, 2)  # (N, H, W, C) в (N, C, H, W)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Вывод картинок после каждой эпохи\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    for i in range(min(BATCH_SIZE, len(images))):\n",
    "        plt.figure()\n",
    "        plt.imshow(images[i].permute(1, 2, 0).detach().numpy())\n",
    "        plt.title(f\"Class: {train_classes[labels[i]]}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f03896",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "for images, _ in test_loader:\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    predictions.extend(predicted.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1a33ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('submission.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['id', 'class'])\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        writer.writerow([idx, train_data.classes[prediction]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de22bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
